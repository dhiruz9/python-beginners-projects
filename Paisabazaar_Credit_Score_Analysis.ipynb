{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e64f15",
   "metadata": {},
   "source": [
    "\n",
    "# Paisabazaar Credit Score Classification — EDA + 5 ML Models\n",
    "\n",
    "**Author:** _Your Name_  \n",
    "**Dataset:** `dataset-2.csv`  \n",
    "**Goal:** Build and compare multiple machine learning models to classify a customer's **Credit_Score** (`Good`, `Standard`, `Poor`) using demographic, financial, and behavioral features.\n",
    "\n",
    "## Business Context\n",
    "Paisabazaar is a financial services company that assists customers in finding and applying for banking and credit products. An integral part of their service is assessing the creditworthiness of individuals, which is crucial for both loan approval and risk management. The credit score of a customer is a significant metric used by financial institutions to determine the likelihood that an individual will default on their loans or credit balances.\n",
    "\n",
    "Accurate classification of credit scores can help Paisabazaar enhance their credit assessment processes, reduce the risk of loan defaults, and offer personalized recommendations. In this project, we analyze and classify credit scores based on customer data, then compare multiple models to identify what works best in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Roadmap (Aligned with Rubric)\n",
    "1. **Summary & Technical Documentation** — This notebook is fully commented and modular.  \n",
    "2. **Exploration** — Head/Tail/Summary and a compact Data Dictionary.  \n",
    "3. **Missing Values** — Identification and handling (imputation).  \n",
    "4. **Conclusions from Data** — Trends & correlations.  \n",
    "5. **Milestones** — EDA, preprocessing, modeling, evaluation.  \n",
    "6. **Visualization** — At least 5 different chart types (matplotlib).  \n",
    "7. **Final Summary** — Model comparison & business implications.  \n",
    "8. **Proper Output Formatting, Modularity, Commented Code** — Included throughout.\n",
    "\n",
    "> **Note:** To keep training time reasonable on large datasets, we sample 20,000 rows for modeling while performing EDA on the full data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a282c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Imports\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Plotting helper to ensure each chart is a fresh figure\n",
    "def new_fig(title=None):\n",
    "    plt.figure()\n",
    "    if title:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82140291",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load Data & Quick Overview\n",
    "We inspect `head()`, `tail()`, `info()`, and `describe()` to understand shape, types, and rough distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Load dataset\n",
    "# ------------------------------\n",
    "df = pd.read_csv(\"dataset-2.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head())\n",
    "display(df.tail(3))\n",
    "print(\"\\nInfo:\")\n",
    "print(df.info())\n",
    "display(df.describe(include='all').transpose().head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c167c6",
   "metadata": {},
   "source": [
    "\n",
    "### Data Dictionary (High-level)\n",
    "> *The exact meanings may vary slightly based on the source; adjust if your course provided official definitions.*\n",
    "\n",
    "- **ID, Customer_ID, Name, SSN, Month**: Identifiers / text; not predictive.  \n",
    "- **Age**: Customer age (years).  \n",
    "- **Occupation**: Job category.  \n",
    "- **Annual_Income, Monthly_Inhand_Salary**: Income-related features.  \n",
    "- **Num_Bank_Accounts, Num_Credit_Card, Num_of_Loan**: Financial product counts.  \n",
    "- **Interest_Rate**: Interest rate on existing loans/credit.  \n",
    "- **Type_of_Loan**: Multi-value text field (drop for baseline due to complexity).  \n",
    "- **Delay_from_due_date, Num_of_Delayed_Payment**: Repayment behavior.  \n",
    "- **Changed_Credit_Limit, Credit_Mix, Outstanding_Debt, Credit_Utilization_Ratio**: Credit behavior metrics.  \n",
    "- **Payment_of_Min_Amount, Payment_Behaviour**: Payment style.  \n",
    "- **Total_EMI_per_month, Amount_invested_monthly, Monthly_Balance**: Expenses/savings.  \n",
    "- **Credit_Score**: **Target** (`Good`, `Standard`, `Poor`).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1c8da1",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Missing Values — Find & Handle\n",
    "We count `NaN` per column and impute: **mean** for numeric features and **most-frequent** for categorical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84151b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Missing values inspection\n",
    "# ------------------------------\n",
    "na_counts = df.isna().sum().sort_values(ascending=False)\n",
    "display(na_counts.head(20))\n",
    "\n",
    "# Simple visualization: bar chart of missing values (top 20)\n",
    "new_fig(\"Top 20 Columns by Missing Values\")\n",
    "na_counts.head(20).plot(kind=\"bar\")\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Missing Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7083a61a",
   "metadata": {},
   "source": [
    "\n",
    "## 3) EDA & Visualizations\n",
    "We generate at least five different plots using **matplotlib**:\n",
    "1. Distribution (histogram) of `Age`  \n",
    "2. Distribution of `Annual_Income`  \n",
    "3. Count plot of `Credit_Score` classes  \n",
    "4. Box plot of `Outstanding_Debt` grouped by `Credit_Score`  \n",
    "5. Correlation heatmap of numeric features  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d7de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Histogram: Age\n",
    "new_fig(\"Age Distribution\")\n",
    "df[\"Age\"].dropna().plot(kind=\"hist\", bins=30)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Histogram: Annual Income\n",
    "new_fig(\"Annual Income Distribution\")\n",
    "df[\"Annual_Income\"].dropna().plot(kind=\"hist\", bins=30)\n",
    "plt.xlabel(\"Annual_Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 3) Count plot: Credit_Score\n",
    "new_fig(\"Credit_Score Counts\")\n",
    "df[\"Credit_Score\"].value_counts().plot(kind=\"bar\")\n",
    "plt.xlabel(\"Credit_Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 4) Box plot: Outstanding_Debt by Credit_Score\n",
    "new_fig(\"Outstanding_Debt by Credit_Score\")\n",
    "df.boxplot(column=\"Outstanding_Debt\", by=\"Credit_Score\", grid=False)\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"Credit_Score\")\n",
    "plt.ylabel(\"Outstanding_Debt\")\n",
    "plt.show()\n",
    "\n",
    "# 5) Correlation heatmap (numeric only)\n",
    "new_fig(\"Correlation Heatmap (Numeric Columns)\")\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "if len(numeric_df.columns) > 1:\n",
    "    corr = numeric_df.corr()\n",
    "    plt.imshow(corr, aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough numeric columns for a heatmap.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bdb8fd",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Preprocessing Pipeline\n",
    "- Drop low-signal identifiers: `ID, Customer_ID, Name, SSN, Month, Type_of_Loan`  \n",
    "- Impute missing values (numeric: mean, categorical: most frequent)  \n",
    "- Encode categoricals with `LabelEncoder` (fast baseline)  \n",
    "- **Optional**: Scale numeric features (helps KNN/SVM)  \n",
    "- Sample **20,000 rows** for training speed, then split train/test.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8c5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Drop identifier-like columns\n",
    "# ------------------------------\n",
    "drop_cols = [\"ID\", \"Customer_ID\", \"Name\", \"SSN\", \"Month\", \"Type_of_Loan\"]\n",
    "df_clean = df.drop(columns=drop_cols, errors=\"ignore\").copy()\n",
    "\n",
    "# ------------------------------\n",
    "# Impute missing values by dtype\n",
    "# ------------------------------\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].dtype == \"object\":\n",
    "        imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "    else:\n",
    "        imp = SimpleImputer(strategy=\"mean\")\n",
    "    df_clean[col] = imp.fit_transform(df_clean[[col]])\n",
    "\n",
    "# ------------------------------\n",
    "# Encode categoricals with LabelEncoder\n",
    "# ------------------------------\n",
    "label_encoders = {}\n",
    "for col in df_clean.select_dtypes(include=\"object\").columns:\n",
    "    le = LabelEncoder()\n",
    "    df_clean[col] = le.fit_transform(df_clean[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# ------------------------------\n",
    "# Train/Test split with sampling for speed\n",
    "# ------------------------------\n",
    "df_clean = df_clean.dropna(subset=[\"Credit_Score\"])\n",
    "\n",
    "sampled = df_clean.sample(n=20000, random_state=42) if len(df_clean) > 20000 else df_clean\n",
    "\n",
    "X = sampled.drop(columns=[\"Credit_Score\"])\n",
    "y = sampled[\"Credit_Score\"]\n",
    "\n",
    "# Optional scaling (helps KNN/SVM); scale numeric columns only\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \" Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b95402",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Modeling — Train Five Classifiers\n",
    "We compare a diverse set of algorithms:\n",
    "\n",
    "1. **Logistic Regression** — linear baseline  \n",
    "2. **Decision Tree** — interpretable rules  \n",
    "3. **Random Forest** — robust ensemble of trees  \n",
    "4. **K-Nearest Neighbors (KNN)** — instance-based learner  \n",
    "5. **Support Vector Machine (SVM)** — maximum margin classifier  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf085d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Initialize models\n",
    "# ------------------------------\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=12, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
    "    \"SVM (Linear)\": SVC(kernel=\"linear\", max_iter=2000)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train, predict, and return metrics dict. Also print classification report and show confusion matrix.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", round(acc, 4))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, preds))\n",
    "\n",
    "    # Confusion Matrix plot\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    new_fig(f\"Confusion Matrix — {name}\")\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(f\"Confusion Matrix — {name}\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = range(len(np.unique(y_test)))\n",
    "    classes = sorted(np.unique(y_test))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"Model\": name, \"Accuracy\": round(acc, 4)}\n",
    "\n",
    "# Train/evaluate all models\n",
    "for name, model in models.items():\n",
    "    metrics = evaluate_model(name, model, X_train, y_train, X_test, y_test)\n",
    "    results.append(metrics)\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eadd29",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Results & Conclusions\n",
    "\n",
    "**Observations:**\n",
    "- Tree ensembles (e.g., **Random Forest**) often perform best on tabular credit data due to non-linear interactions and mixed feature types.  \n",
    "- **Logistic Regression** is a strong, fast baseline; performance depends on linear separability.  \n",
    "- **KNN** can work after scaling but may degrade with high dimensionality.  \n",
    "- **SVM** benefits from scaling; linear kernel is used here for speed and interpretability.  \n",
    "\n",
    "**Business Impact:**\n",
    "- A higher-accuracy model improves risk stratification, reduces default rates, and supports better product recommendations.  \n",
    "- Feature importance from tree-based models can guide policy (e.g., emphasizing repayment behavior or utilization ratios).  \n",
    "\n",
    "**Next Steps:**\n",
    "- Hyperparameter tuning (GridSearch/RandomizedSearch) for top models.  \n",
    "- Try gradient boosting methods (XGBoost/LightGBM/CatBoost).  \n",
    "- Address class imbalance with class weights or resampling if needed.  \n",
    "- Create a simple API (FastAPI/Flask) for deployment.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28de9de",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix — Reusable Utilities\n",
    "Small helpers are provided to keep the notebook modular and clean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa77dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize_dataframe(df, n=3):\n",
    "    \"\"\"Print a compact data summary.\"\"\"\n",
    "    print(\"Shape:\", df.shape)\n",
    "    display(df.head(n))\n",
    "    display(df.tail(n))\n",
    "    print(\"\\nInfo:\")\n",
    "    print(df.info())\n",
    "    display(df.describe(include='all').transpose().head(20))\n",
    "\n",
    "# Example:\n",
    "# summarize_dataframe(df)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
